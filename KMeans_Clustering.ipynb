{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ae73f39",
      "metadata": {
        "id": "8ae73f39"
      },
      "source": [
        "## Q7. Clustering Text Data with K-Means on the 20 Newsgroups Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2aaa37",
      "metadata": {
        "id": "9c2aaa37",
        "outputId": "c880420c-f379-4b3f-cd1f-2ce85d38b630"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c36116",
      "metadata": {
        "id": "a1c36116"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8c7569",
      "metadata": {
        "id": "ab8c7569"
      },
      "outputs": [],
      "source": [
        "# Define the categories to be fetched from the 20 newsgroups dataset\n",
        "categories = [\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "]\n",
        "\n",
        "# Fetch the dataset, removing headers, footers, and quotes\n",
        "df = fetch_20newsgroups(subset='all', categories=categories, shuffle=False, remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0c17bd",
      "metadata": {
        "id": "8e0c17bd",
        "outputId": "68efc271-2b06-4caa-9da1-a8ec9839bf90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK WordNet resource for lemmatization\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Perform Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(df.data)):\n",
        "    word_list = word_tokenize(df.data[i])\n",
        "    lemmatized_doc = \"\"\n",
        "    for word in word_list:\n",
        "        lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
        "    df.data[i] = lemmatized_doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235a37d6",
      "metadata": {
        "id": "235a37d6"
      },
      "outputs": [],
      "source": [
        "# Vectorize the preprocessed text data using TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', min_df=2)\n",
        "X = vectorizer.fit_transform(df.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def9b843",
      "metadata": {
        "id": "def9b843",
        "outputId": "3b249a64-a671-4f19-830b-e225db5ae640"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 2.978s\n"
          ]
        }
      ],
      "source": [
        "# Determine the true number of clusters (categories) in the dataset\n",
        "labels = df.target\n",
        "true_k = len(np.unique(labels))\n",
        "\n",
        "# Initialize k-means clustering with the true number of clusters\n",
        "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\n",
        "\n",
        "# Start the timer for training k-means\n",
        "t0 = time()\n",
        "\n",
        "# Fit k-means to the vectorized data\n",
        "km.fit(X)\n",
        "\n",
        "# Print the time taken for training k-means\n",
        "print(\"done in %0.3fs\" % (time() - t0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de18ee07",
      "metadata": {
        "id": "de18ee07",
        "outputId": "646c9db4-da70-428a-fa2f-476163378f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Homogeneity: 0.318\n",
            "Completeness: 0.378\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the homogeneity score of the clustering\n",
        "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
        "\n",
        "# Calculate and print the completeness score of the clustering\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c804fc",
      "metadata": {
        "id": "f9c804fc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}