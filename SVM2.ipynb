{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PrUx8ov8Jqv"
      },
      "source": [
        "##  Q5. Implement Support Vector Machine (SVM) Classification on 20 Newsgroups Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikF_DNMiKzhx",
        "outputId": "42ac0a53-6a19-4655-e45c-467c659bf188",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to\n",
            "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\names.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('names')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lcj_YUdK49d",
        "outputId": "04eaa03e-4526-4de0-8e85-cdfdc82219a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11314, 11314, 7532, 7532)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "groups = fetch_20newsgroups()\n",
        "data_train = fetch_20newsgroups(subset='train', random_state=21)\n",
        "train_label = data_train.target\n",
        "data_test = fetch_20newsgroups(subset='test', random_state=21)\n",
        "test_label = data_test.target\n",
        "len(data_train.data), len(train_label), len(data_test.data), len(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6selVLa8LB4y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.unique(test_label)\n",
        "from collections import defaultdict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import names  # contains 5001 female names and 2943 male names\n",
        "all_names = names.words()\n",
        "WNL = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_fRzDtWLE06"
      },
      "outputs": [],
      "source": [
        "def clean(data):\n",
        "  cleaned = defaultdict(list)\n",
        "  count = 0\n",
        "  for group in data:\n",
        "     for words in group.split():\n",
        "        if words.isalpha() and words not in all_names:\n",
        "            cleaned[count].append(WNL.lemmatize(words.lower()))\n",
        "     cleaned[count] = ' '.join(cleaned[count])\n",
        "     count +=1\n",
        "  return(list(cleaned.values()))\n",
        "x_train = clean(data_train.data)\n",
        "x_test = clean(data_test.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUI6uHaOLJHn",
        "outputId": "1c418c76-19b4-418b-d067-6e8b44ab0195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time :  25494.792523400043\n",
            "{'C': 1.0}\n",
            "0.7213187626843581\n",
            "0.6259957514604355\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_train = tf.fit_transform(x_train)\n",
        "X_test = tf.transform(x_test)\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "svc_lib = SVC(kernel = 'linear')\n",
        "parameters = {'C' : (0.5,1.0,10,100)}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import timeit\n",
        "grid_search1 =GridSearchCV(svc_lib, parameters, n_jobs = -1, cv = 3)\n",
        "start_time = timeit.default_timer()\n",
        "grid_search1.fit(X_train, train_label)\n",
        "final = timeit.default_timer()-start_time\n",
        "print(\"Execution Time : \",final)\n",
        "print(grid_search1.best_params_)\n",
        "print(grid_search1.best_score_)\n",
        "grid_search_best1 = grid_search1.best_estimator_\n",
        "accur1 = grid_search_best1.score(X_test, test_label)\n",
        "print(accur1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}